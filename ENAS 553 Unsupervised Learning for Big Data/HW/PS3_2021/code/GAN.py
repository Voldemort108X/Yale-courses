import torch
import torch.nn as nn
from torch.optim.adam import Adam
import torchvision.transforms as transforms
import torch.optim as optim
import torchvision.datasets as datasets
# import imageio
import numpy as np
import matplotlib
from torchvision.utils import make_grid, save_image
from torch.utils.data import DataLoader
from matplotlib import pyplot as plt
import os
# from tqdm import tqdm

matplotlib.style.use('ggplot')

criterion = torch.nn.BCELoss()

class Generator(nn.Module):
    def __init__(self, nz):
        super(Generator, self).__init__()
        self.nz = nz # the dimension of the random noise used to seed the Generator
        self.main = nn.Sequential( # nn.sequential is a handy way of combining multiple layers.
            nn.Linear(self.nz, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, 784),
            nn.Tanh(),
        )
    def forward(self, x):
        return self.main(x).view(-1, 1, 28, 28)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.n_input = 784
        self.main = nn.Sequential(
            nn.Linear(self.n_input, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )
    def forward(self, x):
        x = x.view(-1, 784)
        return self.main(x)


def train_discriminator(optimizer, real_data, fake_data):
    """
    Train the discriminator on a minibatch of data.
    INPUTS
        :param optimizer: the optimizer used for training
        :param real_data: the batch of training data
        :param fake_data: the data generated by the generator from random noise
    The discriminator will incur two losses: one from trying to classify the real data, and another from classifying the fake data.
    TODO: Fill in this function.
    It should
    1. Run the discriminator on the real_data and the fake_data
    2. Compute and sum the respective loss terms (described in the assignment)
    3. Backpropogate the loss (e.g. loss.backward()), and perform optimization (e.g. optimizer.step()).
    """
    fake_out = discriminator(fake_data)
    real_out = discriminator(real_data)
    y_fake = torch.zeros(batch_size, 1).to('cuda')
    y_real = torch.ones(batch_size, 1).to('cuda')
    D_real_loss = criterion(real_out, y_real)
    D_fake_loss = criterion(fake_out, y_fake)

    dis_loss = (D_fake_loss + D_real_loss)/2

    # compute the relavant loss
    dis_loss.backward()

    # perform optimization
    optimizer.step()

    # we'll return the loss for book-keeping purposes. (E.g. if you want to make plots of the loss.)
    return dis_loss

def train_generator(optimizer, fake_data):
    """
    Performs a single training step on the generator.
    :param optimizer: the optimizer
    :param fake_data: forgeries, created by the generator from random noise. (Done before calling this function.)
    :return:  the generator's loss
    TODO: Fill in this function
    It should
    1. Run the discriminator on the fake_data
    2. compute the resultant loss for the generator (as described in the assignment)
    3. Backpropagate the loss, and perform optimization
    """
    fake_out = discriminator(fake_data)
    y = torch.ones(batch_size, 1).to('cuda')

    gen_loss = 0.5 * criterion(fake_out, y)
    gen_loss.backward()
    optimizer.step()

    return gen_loss

# import data
batch_size = 100
train_data = datasets.MNIST(
    root='data',
    train=True,
    download=True,
    transform=transforms.ToTensor()
)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

num_epochs = 100
nz = 25# dimension of random noise
generator = Generator(nz).to('cuda')
discriminator = Discriminator().to('cuda')

optimizer_gen = Adam(generator.parameters(), lr=0.0003)
optimizer_dis = Adam(discriminator.parameters(), lr=0.0003)

#TODO: Build a training loop for the GAN
# For each epoch, you'll
# 1. Loop through the training data. For each batch, feed random noise into the generator to generate fake_data of the corresponding size.
# 2. Feed the fake data and real data into the train_discriminator and train_generator functions
# At the end of each epoch, use the below functions to save a grid of generated images.
for epoch in range(num_epochs):
    for data, target in train_loader:
        # perform training
        data = data.to('cuda')

        optimizer_dis.zero_grad()
        noise = torch.randn((batch_size, nz)).to('cuda')
        noise = torch.clamp(noise, 1e-8, 1)
        fake_data = generator(noise)
        dis_loss_train = train_discriminator(optimizer_dis, real_data=data, fake_data=fake_data.detach())

        optimizer_gen.zero_grad()
        noise = torch.randn((batch_size, nz)).to('cuda')
        noise = torch.clamp(noise, 1e-8, 1)
        fake_data = generator(noise)
        gen_loss_train = train_generator(optimizer_gen, fake_data=fake_data)

    if epoch % 10 == 0:
        # train_loss_list.append(train_loss), test_loss_list.append(test_loss)
        print(f" EPOCH {epoch}. Progress: {epoch/num_epochs*100}%. ")
        print(f" Train gen loss: {gen_loss_train.item()}. Train dis loss: {dis_loss_train.item()}") #TODO: implement the evaluate function to provide performance statistics during training.


    # reshape the image tensors into a grid
    generated_img = make_grid(fake_data)
    # save the generated torch tensor images
    save_image(generated_img, f"outputs/gen_img{epoch}.png")

path_save = os.path.join(os.getcwd(), 'models')
torch.save(generator.state_dict(), os.path.join(path_save, 'model_gen.pth'))
torch.save(discriminator.state_dict(), os.path.join(path_save, 'model_dis.pth'))