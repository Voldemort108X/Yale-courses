{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:34.822914Z",
     "iopub.status.busy": "2022-03-29T12:04:34.822455Z",
     "iopub.status.idle": "2022-03-29T12:04:36.803546Z",
     "shell.execute_reply": "2022-03-29T12:04:36.802868Z"
    },
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:38.252501Z",
     "iopub.status.busy": "2022-03-29T12:04:38.252311Z",
     "iopub.status.idle": "2022-03-29T12:04:38.257567Z",
     "shell.execute_reply": "2022-03-29T12:04:38.257070Z"
    },
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "text = open(\"shakespeare.txt\", 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:38.260436Z",
     "iopub.status.busy": "2022-03-29T12:04:38.259875Z",
     "iopub.status.idle": "2022-03-29T12:04:38.263132Z",
     "shell.execute_reply": "2022-03-29T12:04:38.262658Z"
    },
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:38.265863Z",
     "iopub.status.busy": "2022-03-29T12:04:38.265339Z",
     "iopub.status.idle": "2022-03-29T12:04:38.278801Z",
     "shell.execute_reply": "2022-03-29T12:04:38.278233Z"
    },
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:38.281852Z",
     "iopub.status.busy": "2022-03-29T12:04:38.281393Z",
     "iopub.status.idle": "2022-03-29T12:04:39.895775Z",
     "shell.execute_reply": "2022-03-29T12:04:39.895186Z"
    },
    "id": "a86OoYtO01go"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.899395Z",
     "iopub.status.busy": "2022-03-29T12:04:39.899193Z",
     "iopub.status.idle": "2022-03-29T12:04:39.910645Z",
     "shell.execute_reply": "2022-03-29T12:04:39.910120Z"
    },
    "id": "6GMlCe3qzaL9"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX_jbgQqfOi"
   },
   "source": [
    "It converts from tokens to character IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.913982Z",
     "iopub.status.busy": "2022-03-29T12:04:39.913612Z",
     "iopub.status.idle": "2022-03-29T12:04:39.919413Z",
     "shell.execute_reply": "2022-03-29T12:04:39.918890Z"
    },
    "id": "WLv5Q_2TC2pc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.922377Z",
     "iopub.status.busy": "2022-03-29T12:04:39.922200Z",
     "iopub.status.idle": "2022-03-29T12:04:39.929716Z",
     "shell.execute_reply": "2022-03-29T12:04:39.929224Z"
    },
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.932692Z",
     "iopub.status.busy": "2022-03-29T12:04:39.932253Z",
     "iopub.status.idle": "2022-03-29T12:04:39.937025Z",
     "shell.execute_reply": "2022-03-29T12:04:39.936527Z"
    },
    "id": "c2GCh0ySD44s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.939902Z",
     "iopub.status.busy": "2022-03-29T12:04:39.939404Z",
     "iopub.status.idle": "2022-03-29T12:04:39.953271Z",
     "shell.execute_reply": "2022-03-29T12:04:39.952756Z"
    },
    "id": "zxYI-PeltqKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.956023Z",
     "iopub.status.busy": "2022-03-29T12:04:39.955581Z",
     "iopub.status.idle": "2022-03-29T12:04:39.958898Z",
     "shell.execute_reply": "2022-03-29T12:04:39.958364Z"
    },
    "id": "w5apvBDn9Ind"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:39.962063Z",
     "iopub.status.busy": "2022-03-29T12:04:39.961652Z",
     "iopub.status.idle": "2022-03-29T12:04:40.352538Z",
     "shell.execute_reply": "2022-03-29T12:04:40.351878Z"
    },
    "id": "UopbsKi88tm5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.355772Z",
     "iopub.status.busy": "2022-03-29T12:04:40.355346Z",
     "iopub.status.idle": "2022-03-29T12:04:40.359022Z",
     "shell.execute_reply": "2022-03-29T12:04:40.358488Z"
    },
    "id": "qmxrYDCTy-eL"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.362066Z",
     "iopub.status.busy": "2022-03-29T12:04:40.361574Z",
     "iopub.status.idle": "2022-03-29T12:04:40.379324Z",
     "shell.execute_reply": "2022-03-29T12:04:40.378705Z"
    },
    "id": "cjH5v45-yqqH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.382353Z",
     "iopub.status.busy": "2022-03-29T12:04:40.381985Z",
     "iopub.status.idle": "2022-03-29T12:04:40.384966Z",
     "shell.execute_reply": "2022-03-29T12:04:40.384453Z"
    },
    "id": "C-G2oaTxy6km"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.387668Z",
     "iopub.status.busy": "2022-03-29T12:04:40.387314Z",
     "iopub.status.idle": "2022-03-29T12:04:40.398611Z",
     "shell.execute_reply": "2022-03-29T12:04:40.397967Z"
    },
    "id": "BpdjRO2CzOfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PHW902-4oZt"
   },
   "source": [
    "It's easier to see what this is doing if you join the tokens back into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.401520Z",
     "iopub.status.busy": "2022-03-29T12:04:40.401220Z",
     "iopub.status.idle": "2022-03-29T12:04:40.413436Z",
     "shell.execute_reply": "2022-03-29T12:04:40.412730Z"
    },
    "id": "QO32cMWu4a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
    "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
    "\n",
    "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.416533Z",
     "iopub.status.busy": "2022-03-29T12:04:40.416053Z",
     "iopub.status.idle": "2022-03-29T12:04:40.419238Z",
     "shell.execute_reply": "2022-03-29T12:04:40.418579Z"
    },
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.422322Z",
     "iopub.status.busy": "2022-03-29T12:04:40.421814Z",
     "iopub.status.idle": "2022-03-29T12:04:40.426399Z",
     "shell.execute_reply": "2022-03-29T12:04:40.425779Z"
    },
    "id": "WxbDTJTw5u_P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.429495Z",
     "iopub.status.busy": "2022-03-29T12:04:40.429128Z",
     "iopub.status.idle": "2022-03-29T12:04:40.474507Z",
     "shell.execute_reply": "2022-03-29T12:04:40.473983Z"
    },
    "id": "B9iKPXkw5xwa"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.477754Z",
     "iopub.status.busy": "2022-03-29T12:04:40.477361Z",
     "iopub.status.idle": "2022-03-29T12:04:40.499635Z",
     "shell.execute_reply": "2022-03-29T12:04:40.498911Z"
    },
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.503055Z",
     "iopub.status.busy": "2022-03-29T12:04:40.502527Z",
     "iopub.status.idle": "2022-03-29T12:04:40.510116Z",
     "shell.execute_reply": "2022-03-29T12:04:40.509506Z"
    },
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.513485Z",
     "iopub.status.busy": "2022-03-29T12:04:40.513004Z",
     "iopub.status.idle": "2022-03-29T12:04:40.515997Z",
     "shell.execute_reply": "2022-03-29T12:04:40.515400Z"
    },
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# # Number of RNN units\n",
    "rnn_units = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.519035Z",
     "iopub.status.busy": "2022-03-29T12:04:40.518579Z",
     "iopub.status.idle": "2022-03-29T12:04:40.524333Z",
     "shell.execute_reply": "2022-03-29T12:04:40.523677Z"
    },
    "id": "wj8HQ2w8z4iO"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.527421Z",
     "iopub.status.busy": "2022-03-29T12:04:40.527023Z",
     "iopub.status.idle": "2022-03-29T12:04:40.540670Z",
     "shell.execute_reply": "2022-03-29T12:04:40.540151Z"
    },
    "id": "IX58Xj9z47Aw"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:40.543987Z",
     "iopub.status.busy": "2022-03-29T12:04:40.543529Z",
     "iopub.status.idle": "2022-03-29T12:04:43.466950Z",
     "shell.execute_reply": "2022-03-29T12:04:43.466154Z"
    },
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.470802Z",
     "iopub.status.busy": "2022-03-29T12:04:43.470283Z",
     "iopub.status.idle": "2022-03-29T12:04:43.482333Z",
     "shell.execute_reply": "2022-03-29T12:04:43.481650Z"
    },
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      multiple                  16896     \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 multiple                  14168064  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  135234    \n",
      "=================================================================\n",
      "Total params: 14,320,194\n",
      "Trainable params: 14,320,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.489045Z",
     "iopub.status.busy": "2022-03-29T12:04:43.488871Z",
     "iopub.status.idle": "2022-03-29T12:04:43.493536Z",
     "shell.execute_reply": "2022-03-29T12:04:43.493022Z"
    },
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.504091Z",
     "iopub.status.busy": "2022-03-29T12:04:43.503588Z",
     "iopub.status.idle": "2022-03-29T12:04:43.510198Z",
     "shell.execute_reply": "2022-03-29T12:04:43.509521Z"
    },
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'ssance on either side\\nShall be well winged with our chiefest horse.\\nThis, and Saint George to boot! '\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"alCFOqAOfEGySAxhhsln;w?Y,.T!cgPH tEBqdrGltx\\nLiYlobSTNWM.HwNgE$R'IntbnkO3fPfBRDFpWvXBODp&\\ntl-TfrBMLaq\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.513616Z",
     "iopub.status.busy": "2022-03-29T12:04:43.513072Z",
     "iopub.status.idle": "2022-03-29T12:04:43.516440Z",
     "shell.execute_reply": "2022-03-29T12:04:43.515756Z"
    },
    "id": "ZOeWdgxNFDXq"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.519332Z",
     "iopub.status.busy": "2022-03-29T12:04:43.518867Z",
     "iopub.status.idle": "2022-03-29T12:04:43.525706Z",
     "shell.execute_reply": "2022-03-29T12:04:43.525206Z"
    },
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190401, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.528690Z",
     "iopub.status.busy": "2022-03-29T12:04:43.528208Z",
     "iopub.status.idle": "2022-03-29T12:04:43.532507Z",
     "shell.execute_reply": "2022-03-29T12:04:43.532042Z"
    },
    "id": "MAJfS5YoFiHf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.04928"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.535556Z",
     "iopub.status.busy": "2022-03-29T12:04:43.535021Z",
     "iopub.status.idle": "2022-03-29T12:04:43.544717Z",
     "shell.execute_reply": "2022-03-29T12:04:43.544177Z"
    },
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.547927Z",
     "iopub.status.busy": "2022-03-29T12:04:43.547512Z",
     "iopub.status.idle": "2022-03-29T12:04:43.550789Z",
     "shell.execute_reply": "2022-03-29T12:04:43.550278Z"
    },
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.553927Z",
     "iopub.status.busy": "2022-03-29T12:04:43.553400Z",
     "iopub.status.idle": "2022-03-29T12:04:43.556240Z",
     "shell.execute_reply": "2022-03-29T12:04:43.555725Z"
    },
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:04:43.559198Z",
     "iopub.status.busy": "2022-03-29T12:04:43.558716Z",
     "iopub.status.idle": "2022-03-29T12:06:31.473361Z",
     "shell.execute_reply": "2022-03-29T12:06:31.472701Z"
    },
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 13s 74ms/step - loss: 2.7186\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 1.8941\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 1.6209\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 1.4651\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.3630\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 1.2850\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 1.2119\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 1.1350\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.0498\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 0.9528\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 0.8448\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.7301\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 0.6218\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.5230\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.4440\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.3878\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 0.3496\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.3300\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.3261\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.3411\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.3684\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.3965\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.4242\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.4489\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.4787\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 0.5141\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.5516\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.5919\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.6474\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.7009\n",
      "Epoch 31/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 0.7612\n",
      "Epoch 32/50\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 0.8122\n",
      "Epoch 33/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.8665\n",
      "Epoch 34/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 0.9134\n",
      "Epoch 35/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.9745\n",
      "Epoch 36/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.0459\n",
      "Epoch 37/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.0872\n",
      "Epoch 38/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.1368\n",
      "Epoch 39/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.1729\n",
      "Epoch 40/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.2044\n",
      "Epoch 41/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.2496\n",
      "Epoch 42/50\n",
      "172/172 [==============================] - 14s 78ms/step - loss: 1.2707\n",
      "Epoch 43/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3057\n",
      "Epoch 44/50\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 1.3014\n",
      "Epoch 45/50\n",
      "172/172 [==============================] - 13s 77ms/step - loss: 1.2924\n",
      "Epoch 46/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.3104\n",
      "Epoch 47/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.3356\n",
      "Epoch 48/50\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.3570\n",
      "Epoch 49/50\n",
      "172/172 [==============================] - 13s 76ms/step - loss: 1.6279\n",
      "Epoch 50/50\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.6256\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:06:31.477418Z",
     "iopub.status.busy": "2022-03-29T12:06:31.477017Z",
     "iopub.status.idle": "2022-03-29T12:06:31.485553Z",
     "shell.execute_reply": "2022-03-29T12:06:31.485020Z"
    },
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:06:31.488377Z",
     "iopub.status.busy": "2022-03-29T12:06:31.487832Z",
     "iopub.status.idle": "2022-03-29T12:06:31.497705Z",
     "shell.execute_reply": "2022-03-29T12:06:31.497230Z"
    },
    "id": "fqMOuDutnOxK"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:06:31.500839Z",
     "iopub.status.busy": "2022-03-29T12:06:31.500317Z",
     "iopub.status.idle": "2022-03-29T12:06:34.153550Z",
     "shell.execute_reply": "2022-03-29T12:06:34.152908Z"
    },
    "id": "ST7PSyk9t1mT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Now, my tankent, and the howniss's he prescue from and kin.\n",
      "And for charthalicing good who or boin, and shat'st trou't for I.\n",
      "You who is Might ray on that I bears it pase\n",
      "Toges it s we beark which thee pleasaring that? He she\n",
      "enem, and that here bate.\n",
      "\n",
      "MENCENTES:\n",
      "Why, you loventis of Hail, unsacked'd it\n",
      "these he of I am dukath.\n",
      "\n",
      "QUEEN:\n",
      "Mistake most Hear up the the tutess, I.\n",
      "\n",
      "KING HENRY EDWARD IV:\n",
      "This Marcius:\n",
      "Then I what beoke you day; and thting redomm\n",
      "That my braise I give?\n",
      "\n",
      "AMPEBON:\n",
      "They sue, I have;\n",
      "Whiapes it wife, theirree appitious thine?\n",
      "This in his in him teed,\n",
      "Then is cromentine sofroms and pates that's friends as ther:\n",
      "Year, is you will her see.\n",
      "\n",
      "Therear:\n",
      "My Argrarow' way; let's hereinn loan!\n",
      "Nor dyesen men road our auntereign.\n",
      "\n",
      "Pecont gentle lady hend, not let'd as sucate Groas!\n",
      "Toid youre pronacury, being sofels wrother; your pewinch air,\n",
      "But for the fiditis prouls, brophering to seebthences\n",
      "To any our his ears me. \n",
      "DUKE OF YORK:\n",
      "Whesencomented,\n",
      "So feencor a truit orde. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 9.293364524841309\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
